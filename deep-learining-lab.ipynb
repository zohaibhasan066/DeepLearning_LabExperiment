{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experiment 1","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# Importing required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# 1. Pandas - Creating a DataFrame and performing basic operations\nprint(\"Pandas Example:\")\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'Score': [85, 90, 88]\n}\ndf = pd.DataFrame(data)\nprint(\"DataFrame:\\n\", df)\n\n# Selecting a column\nprint(\"\\nSelecting 'Name' column:\\n\", df['Name'])\n\n# Adding a new column\ndf['Passed'] = df['Score'] > 80\nprint(\"\\nUpdated DataFrame:\\n\", df)\n\n# 2. NumPy - Creating arrays and basic operations\nprint(\"\\nNumPy Example:\")\narray = np.array([1, 2, 3, 4, 5])\nprint(\"Array:\", array)\n\n# Operations\nprint(\"Array + 10:\", array + 10)\nprint(\"Mean of array:\", np.mean(array))\n\n# 3. Plotting - Creating a simple plot\nprint(\"\\nPlotting Example:\")\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 6, 8, 10]\nplt.plot(x, y, label=\"y=2x\", color='blue', marker='o')\nplt.title(\"Line Plot Example\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# 4. Create a list and perform basic operations\nprint(\"\\nList Example:\")\nmy_list = [10, 20, 30, 40, 50]\nprint(\"Original List:\", my_list)\n\n# Add an element\nmy_list.append(60)\nprint(\"List after adding 60:\", my_list)\n\n# Remove an element\nmy_list.remove(20)\nprint(\"List after removing 20:\", my_list)\n\n# 5. Scikit-learn (Scilear) - Performing a basic regression\nprint(\"\\nScikit-learn Example:\")\n# Short dataset for regression\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([1.5, 3.0, 4.5, 6.0, 7.5])\n\n# Create a Linear Regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predict values\npredictions = model.predict(X)\nprint(\"Predictions:\", predictions)\n\n# Plot regression line\nplt.scatter(X, y, color='red', label=\"Data Points\")\nplt.plot(X, predictions, color='blue', label=\"Regression Line\")\nplt.title(\"Linear Regression Example\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T11:53:02.331774Z","iopub.execute_input":"2024-11-21T11:53:02.332196Z","iopub.status.idle":"2024-11-21T11:53:02.844367Z","shell.execute_reply.started":"2024-11-21T11:53:02.332162Z","shell.execute_reply":"2024-11-21T11:53:02.843251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 2","metadata":{"execution":{"iopub.status.busy":"2024-11-21T12:08:59.708616Z","iopub.execute_input":"2024-11-21T12:08:59.709058Z","iopub.status.idle":"2024-11-21T12:08:59.721038Z","shell.execute_reply.started":"2024-11-21T12:08:59.709022Z","shell.execute_reply":"2024-11-21T12:08:59.719761Z"}}},{"cell_type":"code","source":"import numpy as np\n\n# Define the activation function (step function)\ndef step_function(x):\n    return np.where(x >= 0, 1, 0)\n\n# Define the single-layer perceptron class\nclass SingleLayerPerceptron:\n    def __init__(self, input_size, learning_rate=0.01):\n        self.weights = np.random.rand(input_size)  # Initialize weights randomly\n        self.bias = np.random.rand()              # Initialize bias randomly\n        self.learning_rate = learning_rate\n\n    def predict(self, inputs):\n        # Compute the linear combination\n        linear_output = np.dot(inputs, self.weights) + self.bias\n        # Apply the activation function\n        return step_function(linear_output)\n\n    def train(self, X, y, epochs=10):\n        for epoch in range(epochs):\n            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n            for i in range(len(X)):\n                # Make a prediction\n                prediction = self.predict(X[i])\n                # Compute the error\n                error = y[i] - prediction\n                # Update weights and bias\n                self.weights += self.learning_rate * error * X[i]\n                self.bias += self.learning_rate * error\n                # Print the progress\n                print(f\"Sample {i+1}, Error: {error}, Weights: {self.weights}, Bias: {self.bias}\")\n\n# Example dataset (AND logic gate)\nX = np.array([\n    [0, 0],\n    [0, 1],\n    [1, 0],\n    [1, 1]\n])\ny = np.array([0, 0, 0, 1])  # AND gate output\n\n# Initialize and train the perceptron\nperceptron = SingleLayerPerceptron(input_size=2, learning_rate=0.1)\nperceptron.train(X, y, epochs=10)\n\n# Test the trained perceptron\nprint(\"\\nTesting the trained perceptron:\")\nfor i in range(len(X)):\n    print(f\"Input: {X[i]}, Predicted: {perceptron.predict(X[i])}, Actual: {y[i]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:23:08.809642Z","iopub.execute_input":"2024-11-21T12:23:08.810056Z","iopub.status.idle":"2024-11-21T12:23:08.834803Z","shell.execute_reply.started":"2024-11-21T12:23:08.810019Z","shell.execute_reply":"2024-11-21T12:23:08.833561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 3","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nclass ADALINE:\n    def __init__(self, input_size, learning_rate=0.01):\n        \"\"\"\n        Initialize the ADALINE model.\n        \n        Parameters:\n        input_size (int): Number of features in the input data.\n        learning_rate (float): Learning rate for weight updates.\n        \"\"\"\n        self.weights = np.zeros(input_size)\n        self.bias = 0.0\n        self.learning_rate = learning_rate\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions using the current weights and bias.\n        \n        Parameters:\n        X (array): Input feature array.\n        \n        Returns:\n        array: Predicted values (linear output).\n        \"\"\"\n        return np.dot(X, self.weights) + self.bias\n\n    def train(self, X, y, epochs=10):\n        \"\"\"\n        Train the ADALINE model using the LMS rule.\n        \n        Parameters:\n        X (array): Input features (2D array).\n        y (array): Target values (1D array).\n        epochs (int): Number of training iterations over the dataset.\n        \"\"\"\n        for epoch in range(epochs):\n            print(f\"Epoch {epoch + 1}/{epochs}\")\n            \n            # Compute predictions for all samples\n            predictions = self.predict(X)\n            \n            # Calculate errors\n            errors = y - predictions\n            \n            # Update weights and bias using the LMS rule\n            self.weights += self.learning_rate * np.dot(X.T, errors)\n            self.bias += self.learning_rate * errors.sum()\n            \n            # Compute mean squared error\n            mse = (errors**2).mean()\n            print(f\"Mean Squared Error: {mse:.4f}\")\n            print(f\"Weights: {self.weights}, Bias: {self.bias}\\n\")\n\n\n# Input dataset: AND Gate\nX = np.array([\n    [0, 0],  # Input 1\n    [0, 1],  # Input 2\n    [1, 0],  # Input 3\n    [1, 1]   # Input 4\n])\n\ny = np.array([0, 0, 0, 1])  # Target output for AND Gate\n\n# Initialize ADALINE model\nadaline = ADALINE(input_size=2, learning_rate=0.1)\n\n# Train the ADALINE model\nadaline.train(X, y, epochs=10)\n\n# Test the trained ADALINE model\nprint(\"Testing the trained ADALINE:\")\nfor sample, label in zip(X, y):\n    prediction = adaline.predict(sample)\n    output = 1 if prediction >= 0.5 else 0  # Apply threshold for binary classification\n    print(f\"Input: {sample}, Predicted: {output}, Actual: {label}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:35:17.849878Z","iopub.execute_input":"2024-11-21T12:35:17.850354Z","iopub.status.idle":"2024-11-21T12:35:17.867112Z","shell.execute_reply.started":"2024-11-21T12:35:17.850277Z","shell.execute_reply":"2024-11-21T12:35:17.865721Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 4 ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nclass BackpropagationNetwork:\n    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n        \"\"\"Initialize the Backpropagation Network.\"\"\"\n        self.hidden_weights = np.random.randn(input_size, hidden_size) * 0.01\n        self.hidden_bias = np.zeros((1, hidden_size))\n        self.output_weights = np.random.randn(hidden_size, output_size) * 0.01\n        self.output_bias = np.zeros((1, output_size))\n        self.learning_rate = learning_rate\n\n    def sigmoid(self, x):\n        \"\"\"Sigmoid activation function.\"\"\"\n        return 1 / (1 + np.exp(-x))\n\n    def sigmoid_derivative(self, x):\n        \"\"\"Derivative of the sigmoid function.\"\"\"\n        return x * (1 - x)\n\n    def forward(self, X):\n        \"\"\"Forward propagation.\"\"\"\n        self.hidden_layer_input = np.dot(X, self.hidden_weights) + self.hidden_bias\n        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n        self.output_layer_input = np.dot(self.hidden_layer_output, self.output_weights) + self.output_bias\n        self.output_layer_output = self.sigmoid(self.output_layer_input)\n        return self.output_layer_output\n\n    def backward(self, X, y, output):\n        \"\"\"Backward propagation to update weights and biases.\"\"\"\n        output_error = y - output\n        output_delta = output_error * self.sigmoid_derivative(output)\n        \n        hidden_error = np.dot(output_delta, self.output_weights.T)\n        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer_output)\n\n        self.output_weights += np.dot(self.hidden_layer_output.T, output_delta) * self.learning_rate\n        self.output_bias += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate\n\n        self.hidden_weights += np.dot(X.T, hidden_delta) * self.learning_rate\n        self.hidden_bias += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate\n\n    def train(self, X, y, epochs):\n        \"\"\"Train the Backpropagation Network.\"\"\"\n        for epoch in range(epochs):\n            output = self.forward(X)\n            self.backward(X, y, output)\n            loss = np.mean(np.square(y - output))\n            if (epoch + 1) % 10 == 0:  # Print loss every 10 epochs\n                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n\n    def predict(self, X):\n        \"\"\"Make predictions with the trained network.\"\"\"\n        return self.forward(X)\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# One-hot encoding for the target labels\ny_onehot = np.zeros((y.size, y.max() + 1))\ny_onehot[np.arange(y.size), y] = 1\n\n# Standardize the input features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.3, random_state=42)\n\n# Initialize and train the Backpropagation Network\nbpn = BackpropagationNetwork(input_size=X_train.shape[1], hidden_size=5, output_size=3, learning_rate=0.1)\nbpn.train(X_train, y_train, epochs=200)\n\n# Make predictions and evaluate the model\npredictions = bpn.predict(X_test)\npredictions = np.argmax(predictions, axis=1)\ny_test_labels = np.argmax(y_test, axis=1)\n\naccuracy = accuracy_score(y_test_labels, predictions)\nprint(f\"\\nModel Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T12:45:02.971952Z","iopub.execute_input":"2024-11-21T12:45:02.972399Z","iopub.status.idle":"2024-11-21T12:45:03.024650Z","shell.execute_reply.started":"2024-11-21T12:45:02.972360Z","shell.execute_reply":"2024-11-21T12:45:03.023459Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 5","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nclass RBFNN:\n    def __init__(self, sigma):\n        self.sigma = sigma\n        self.centers = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n        self.weights = None\n\n    def _gaussian(self, x, c):\n        return np.exp(-np.linalg.norm(x - c) ** 2 / (2 * self.sigma ** 2))\n\n    def _calculate_activation(self, X):\n        activations = np.zeros((X.shape[0], self.centers.shape[0]))\n        for i, center in enumerate(self.centers):\n            for j, x in enumerate(X):\n                activations[j, i] = self._gaussian(x, center)\n        return activations\n\n    def fit(self, X, y):\n        # Calculate activations\n        activations = self._calculate_activation(X)\n\n        # Initialize and solve for weights\n        self.weights = np.linalg.pinv(activations.T @ activations) @ activations.T @ y\n\n    def predict(self, X):\n        if self.weights is None:\n            raise ValueError(\"Model not trained yet. Call fit method first.\")\n        \n        activations = self._calculate_activation(X)\n        return activations @ self.weights\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Define XOR dataset\n    X = np.array([[0.1, 0.1], [0.1, 0.9], [0.9, 0.1], [0.9, 0.9]])\n    y = np.array([0, 1, 1, 0])\n\n    # Initialize and train RBFNN\n    rbfnn = RBFNN(sigma=0.1)\n    rbfnn.fit(X, y)\n\n    # Predict\n    predictions = rbfnn.predict(X)\n    print(\"Predictions:\", predictions)\n\n    # Calculate mean squared error\n    mse = np.mean((predictions - y) ** 2)\n    print(\"Mean Squared Error:\", mse)\n\n    # Plot the results\n    plt.scatter(X[:, 0], X[:, 1], c=predictions, cmap='viridis')\n    plt.colorbar(label='Predicted Output')\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.title('RBFN Predictions for XOR ')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T04:36:35.012937Z","iopub.execute_input":"2024-11-22T04:36:35.013371Z","iopub.status.idle":"2024-11-22T04:36:35.389429Z","shell.execute_reply.started":"2024-11-22T04:36:35.013335Z","shell.execute_reply":"2024-11-22T04:36:35.388024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 6","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve, train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.neural_network import MLPClassifier\n\n# 1. Generate a synthetic dataset\nX, y = make_classification(\n    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, \n    random_state=42, n_classes=2\n)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2. Define a simple neural network model\nmodel = MLPClassifier(hidden_layer_sizes=(50, 30), max_iter=500, random_state=42)\n\n# 3. Compute learning curves\ntrain_sizes, train_scores, test_scores = learning_curve(\n    model, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)\n)\n\n# 4. Calculate mean and standard deviation for train and validation scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# 5. Plot learning curves\nplt.figure(figsize=(10, 6))\nplt.plot(train_sizes, train_mean, label=\"Training Accuracy\", color=\"blue\", marker=\"o\")\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"blue\", alpha=0.2)\n\nplt.plot(train_sizes, test_mean, label=\"Validation Accuracy\", color=\"green\", marker=\"o\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"green\", alpha=0.2)\n\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\")\nplt.ylabel(\"Accuracy\")\nplt.legend(loc=\"best\")\nplt.grid()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T04:38:07.427055Z","iopub.execute_input":"2024-11-22T04:38:07.427558Z","iopub.status.idle":"2024-11-22T04:38:49.434338Z","shell.execute_reply.started":"2024-11-22T04:38:07.427515Z","shell.execute_reply":"2024-11-22T04:38:49.433222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 7","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import mnist\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Load and preprocess the MNIST dataset\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# Normalize the data to range [0, 1]\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# Reshape data to include channel dimension (required for CNN)\nX_train = X_train[..., np.newaxis]\nX_test = X_test[..., np.newaxis]\n\n# One-hot encode the labels\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\n# 2. Build the CNN model\nmodel = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\n# 3. Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# 4. Train the model\nhistory = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n\n# 5. Evaluate the model\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\nprint(f\"Test accuracy: {test_accuracy:.4f}\")\n\n# 6. Visualize some predictions\npredictions = model.predict(X_test)\n\nplt.figure(figsize=(10, 10))\nfor i in range(16):\n    plt.subplot(4, 4, i + 1)\n    plt.imshow(X_test[i].squeeze(), cmap='gray')\n    plt.title(f\"Predicted: {np.argmax(predictions[i])}\\nTrue: {np.argmax(y_test[i])}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n\n# 7. Plot learning curves\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy Curve')\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss Curve')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T04:44:27.363366Z","iopub.execute_input":"2024-11-22T04:44:27.364028Z","iopub.status.idle":"2024-11-22T04:47:09.428235Z","shell.execute_reply.started":"2024-11-22T04:44:27.363983Z","shell.execute_reply":"2024-11-22T04:47:09.426955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 8","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\n\n# 1. Load and preprocess the IMDB dataset\nmax_features = 10000  # Vocabulary size\nmax_len = 200         # Maximum review length (after padding)\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n\n# Pad sequences to ensure consistent input length\nX_train = pad_sequences(X_train, maxlen=max_len)\nX_test = pad_sequences(X_test, maxlen=max_len)\n\n# 2. Build the RNN model\nmodel = models.Sequential([\n    layers.Embedding(max_features, 64, input_length=max_len),  # Embedding layer\n    layers.SimpleRNN(64, return_sequences=False, activation='tanh'),  # RNN layer\n    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n])\n\n# 3. Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# 4. Train the model\nhistory = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n\n# 5. Evaluate the model\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\nprint(f\"Test accuracy: {test_accuracy:.4f}\")\n\n# 6. Visualize some predictions\npredictions = model.predict(X_test[:10])\n\nfor i in range(10):\n    print(f\"Review {i + 1}: {'Positive' if predictions[i] > 0.5 else 'Negative'} | True Label: {'Positive' if y_test[i] == 1 else 'Negative'}\")\n\n# 7. Plot learning curves\nplt.figure(figsize=(12, 5))\n\n# Plot accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy Curve')\n\n# Plot loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss Curve')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T05:01:32.297190Z","iopub.execute_input":"2024-11-22T05:01:32.297530Z","iopub.status.idle":"2024-11-22T05:02:26.263430Z","shell.execute_reply.started":"2024-11-22T05:01:32.297502Z","shell.execute_reply":"2024-11-22T05:02:26.262445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 9","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\n# 1. Load and preprocess the CIFAR-10 dataset\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n# Normalize pixel values to [0, 1] range\nX_train = X_train.astype('float32') / 255.0\nX_test = X_test.astype('float32') / 255.0\n\n# One-hot encode labels\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\n\n# 2. Build the Advanced CNN model\nmodel = models.Sequential([\n    # First Convolutional Block\n    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),\n    layers.BatchNormalization(),\n    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    # Second Convolutional Block\n    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    # Third Convolutional Block\n    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPooling2D((2, 2)),\n    layers.Dropout(0.25),\n\n    # Fully Connected Layers\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(10, activation='softmax')  # Output layer for 10 classes\n])\n\n# 3. Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# 4. Train the model\nhistory = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_split=0.2)\n\n# 5. Evaluate the model\ntest_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# 6. Plot learning curves\nplt.figure(figsize=(12, 5))\n\n# Accuracy curve\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Accuracy Curve')\n\n# Loss curve\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Loss Curve')\n\nplt.show()\n\n# 7. Display predictions\nclass_names = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\npredictions = model.predict(X_test[:10])\n\nplt.figure(figsize=(10, 5))\nfor i in range(10):\n    plt.subplot(2, 5, i + 1)\n    plt.imshow(X_test[i])\n    plt.title(f\"Pred: {class_names[predictions[i].argmax()]}\\nTrue: {class_names[y_test[i].argmax()]}\")\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T05:13:12.123879Z","iopub.execute_input":"2024-11-22T05:13:12.124490Z","iopub.status.idle":"2024-11-22T05:14:25.335495Z","shell.execute_reply.started":"2024-11-22T05:13:12.124448Z","shell.execute_reply":"2024-11-22T05:14:25.334662Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment 10","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.data import find\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport os\n\n# Step 1: Create a directory for NLTK data if not already present\nnltk_data_path = '/kaggle/working/nltk_data'\nif not os.path.exists(nltk_data_path):\n    os.makedirs(nltk_data_path)\n\n# Ensure NLTK uses this directory\nnltk.data.path.append(nltk_data_path)\n\n# Step 2: Manually download resources (to ensure they go into the correct path)\nnltk.download('punkt', download_dir=nltk_data_path)\nnltk.download('stopwords', download_dir=nltk_data_path)\nnltk.download('wordnet', download_dir=nltk_data_path)\nnltk.download('omw-1.4', download_dir=nltk_data_path)\n\n# Check if wordnet is downloaded correctly\ntry:\n    find('corpora/wordnet.zip')\n    print(\"WordNet corpus found!\")\nexcept LookupError:\n    print(\"WordNet corpus not found!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T05:30:26.763574Z","iopub.execute_input":"2024-11-22T05:30:26.763915Z","iopub.status.idle":"2024-11-22T05:30:26.866976Z","shell.execute_reply.started":"2024-11-22T05:30:26.763885Z","shell.execute_reply":"2024-11-22T05:30:26.866136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\n# Download necessary NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Load spaCy model for lemmatization\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Sample text\ntext = \"\"\"\nNatural Language Processing (NLP) is a sub-field of artificial intelligence that focuses on the interaction between \ncomputers and humans through natural language. It involves text analysis, machine translation, sentiment analysis, \nand much more. NLP has applications in chatbots, virtual assistants, and automated text processing systems.\n\"\"\"\n\n# 1. Tokenization using NLTK\ntokens = word_tokenize(text)\nprint(\"Tokens:\", tokens)\n\n# 2. Stopword Removal using NLTK\nstop_words = set(stopwords.words('english'))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\nprint(\"\\nTokens after stopword removal:\", filtered_tokens)\n\n# 3. Stemming using NLTK\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\nprint(\"\\nStemmed Tokens:\", stemmed_tokens)\n\n# 4. Lemmatization using spaCy\nlemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(filtered_tokens)).doc]\nprint(\"\\nLemmatized Tokens:\", lemmatized_tokens)\n\n# 5. Word Frequency Analysis\nword_counts = Counter(lemmatized_tokens)\nprint(\"\\nWord Frequencies:\", word_counts)\n\n# 6. Word Cloud Visualization\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(lemmatized_tokens))\n\n# Plot Word Cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title(\"Word Cloud Visualization\", fontsize=16)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T05:33:12.549828Z","iopub.execute_input":"2024-11-22T05:33:12.550691Z","iopub.status.idle":"2024-11-22T05:33:18.542550Z","shell.execute_reply.started":"2024-11-22T05:33:12.550623Z","shell.execute_reply":"2024-11-22T05:33:18.541729Z"}},"outputs":[],"execution_count":null}]}